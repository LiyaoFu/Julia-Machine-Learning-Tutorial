#=
This is an basic RNN-implementation
that trains single layer RNN

We generate data, test, train,
and evaluate visually
=#

using Flux
using Flux: @epochs
using Plots

#parameters used for testing
iterate = 500
epoch_time = 200
range = 1000
test_change = 5

#=
This is the train data generated.
We generate random numbers in range 1-1000 
for 500 number of times
=#
generated0 = []
for i in 1:iterate
    append!(generated0,[[[rand(1:range)]]])
end

# train label for generated train data
generated_label = (x -> sum(x)).(generated0)

# the type of train data is adjusted  
generated = (x -> Vector{Vector{Float32}}(x)).(generated0)

#=
This is the test data generated, 
It is generated by multiplying the value of 
train_data by 5 times
=#
test0 = []
for obj in generated0
    append!(test0, [[[test_change*sum(sum(obj))]]])
end

# test label for generated test data 
test_label = (x -> sum(x)).(test0)

# the type of test data is adjusted
test = (x -> Vector{Vector{Float32}}(x)).(test0)

#=
use flux to create a single linear RNN model
=#
our = Flux.RNN(1, 1, (x->x))

#=
evalute the result 
=#
function eval(x)
  # the last element is the result we want 
  out = our.(x)[end]
  # make sure the flux is not polluted  
  Flux.reset!(our)
  out
end

#=
loss function for flux.train
=#
function loss(x, y)
    out = abs(sum((eval(x).- y)))
    out
end

#
params = Flux.params(our)

#
optimizer = Flux.ADAM()

println("training loss = ", sum(loss.(generated, generated_label)))
println("Test loss without training = ", sum(loss.(test, test_label)))

#=
we zip the train data and train label
together to meet the datatype of loss
=#
zipped_data = []
for i in 1:length(generated)
    obj1 = generated[i]
    obj2 = generated_label[i]
    append!(zipped_data, [(obj1, obj2)])
end

# train the RNN model 
@epochs epoch_time Flux.train!(loss, params, zipped_data, optimizer, cb = () -> println("training"))
 
println("Test loss after training = ", sum(loss.(test, test_label)))
